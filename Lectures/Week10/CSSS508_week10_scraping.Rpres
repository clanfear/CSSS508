CSSS 508, Week 10: Social Media and Text Mining
====================================================================================
author: Charles Lanfear
date: November 29th, 2017
transition: linear
width: 1440
height: 900


Topics
====================================================================================

Today we will be covering:

* Important terminology in Social Media and Text Mining
* Collecting Twitter data with `SocialMediaLab` and `streamR`
* Mining text with `tm` 

Collecting Twitter Data
====================================================================================
type: section

Some Terminology
====================================================================================
incremental: true

* Application Program Interface (API): A type of computer interface that exists as the
"native" method of communication between computers, often via http (usable via `httr` package).
   + R packages that interface with websites and databases typically use APIs.
   + APIs make accessing data easy while allowing websites to control access.
* XML and JSON: File formats commonly used for commucation via APIs.
   + Can be parsed with the `XML` and `jsonlite` packages.
* Scrape: The act of extracting data from sources designed to be human readable rather
than machine readable.
   + Technically if one goes through an API, it is not *scraping* data.
   + R can be used to scrape from sources lacking an API but *technical knowledge is required*.
* Rate Limit: A web service's limit to the number of pieces of data you can download in
a given period of time (use `Sys.sleep()` to deal with this).

Twitter
====================================================================================
incremental: true

Twitter has two APIs for obtaining data from tweets and users:

* Twitter REST API
    + Allows reading and writing Twitter data.
    + Can obtain tweets from specific dates and places easily, but doesn't get *everything*
    + Good for obtaining lots of data, data over time, etc.
* Twitter Streaming API:
    + Obtains tweets as they are being posted.
    + Only gives you tweets that were posted while you are watching.
    + Good for following trends, live events, etc.

Set Up
====================================================================================

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_knit$set(root.dir="C:/Users/cclan/OneDrive/GitHub/CSSS508/Lectures/Week10/")
setwd("C:/Users/cclan/OneDrive/GitHub/CSSS508/Lectures/Week10/")
library(SocialMediaLab)
library(tidyverse)
library(twitteR)
library(ROAuth)
library(grid)
library(stringr)
library(streamR)

load("twitCred.Rdata")

api_key <- twitCred$consumerKey
api_secret <- twitCred$consumerSecret
access_token <- twitCred$oauthKey
access_token_secret <- twitCred$oauthSecret

reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL<- "https://api.twitter.com/oauth/access_token"
authURL<- "https://api.twitter.com/oauth/authorize"


```

```{r false_Setup, eval=FALSE}
library(SocialMediaLab); library(tidyverse); library(stringr)
library(twitteR); library(ROAuth); library(grid); library(streamR)

api_key <- "API KEY GOES HERE"
api_secret <- "API SECRET GOES HERE"
access_token <- "ACCESS TOKEN GOES HERE"
access_token_secret <- "ACCESS TOKEN SECRET GOES HERE"

reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL<- "https://api.twitter.com/oauth/access_token"
authURL<- "https://api.twitter.com/oauth/authorize"
```


Social Media Lab
====================================================================================
type: section


Social Media Lab
====================================================================================
incremental: true

Social Media Lab provides a "one-stop shop" for accessing social media data and 
transforming it for network analysis techniques. It works with:

* **Twitter** (Tweets from REST API)
* **Facebook** (Pages and public posts)
* **Instagram** (Public pictures and comments)
* **YouTube** (Videos and comments)

Data from all of these can be collected and analyzed using the same syntax!
[See this introduction to see exactly how much it can do](http://www.academia.edu/19064267/Absolute_Beginners_Guide_to_the_SocialMediaLab_package_in_R).

We will only cover Twitter today, but Facebook and Youtube are similar. Instagram is
more difficult to get API access to, but possible in theory.

SML Work Flow
====================================================================================
incremental: true

Social Media Lab has a simple workflow for collecting data:

1. `Authenticate()` using your credentials
2. Pipe authentication to...
3. `Collect()` where you can specify search parameters.
4. Then either...
   + Use the data as they are for text analysis
   + Use `Create()` to generate network data

It will perform the search and process the data into a usable data frame. This beats
handling `JSON` files yourself!


Twitter's REST API
====================================================================================

```{r REST_API, include=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
tweets_Trump <- Authenticate("twitter", api_key, api_secret,
                             access_token, access_token_secret) %>%
  Collect(searchTerm="Trump", language="en", verbose=TRUE) %>%
  as_tibble()
```

```{r REST_API_display, eval=FALSE}
tweets_Trump <- Authenticate("twitter", api_key, api_secret,
                             access_token, access_token_secret) %>% 
  Collect(searchTerm="Trump", language="en", verbose=TRUE) %>% 
  as_tibble()

tweets_Trump %>% arrange(retweetCount) %>% select(text)
```

```{r REST_API_eval, echo=FALSE}
tweets_Trump %>% arrange(retweetCount) %>%
  select(text)
```

Twitter's REST API 2
====================================================================================

```{r REST_API_2, include=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
tweets_UW <- Authenticate("twitter", api_key, api_secret,
                                access_token, access_token_secret) %>%
  Collect(geocode="47.6552083,-122.30833,2mi", searchTerm=" ",
          language="en", verbose=TRUE)

tweets_UW %>% arrange(retweetCount) %>%
  select(text)
```

```{r REST_API_2_display, eval=FALSE}
tweets_UW <- Authenticate("twitter", api_key, api_secret,
                                access_token, access_token_secret) %>%
  Collect(geocode="47.6552083,-122.30833,2mi", searchTerm=" ",
          language="en", verbose=TRUE)

tweets_UW %>% arrange(retweetCount) %>% select(text)
```


```{r REST_API_2_eval, echo=FALSE}
tweets_UW %>% arrange(retweetCount) %>% select(text)
```


Twitter Stream with streamR
====================================================================================

There are a few packages that can access the Twitter Stream API including `streamR`.
Since the Stream API is less commonly used in Social Science, the packages are a bit 
"rough around the edges" and require more finicky setup.

This is likely to change over time as more people make use of the Stream API.


streamR Setup
====================================================================================

```{r streamR_setup, eval=FALSE}
download.file(url="http://curl.haxx.se/ca/cacert.pem",
              destfile="cacert.pem")
twitCred <- OAuthFactory$new(consumerKey=api_key,
                             consumerSecret=api_secret,
                             requestURL=reqURL,
                             accessURL=accessURL,
                             authURL=authURL)

# insert the number in the R console after you run this
twitCred$handshake(cainfo = 
            system.file("CurlSSL", "cacert.pem", package = "RCurl"))

# Then save this so you don't need to do it again
save(twitCred, file = "twitCred.Rdata")
```


streamR
====================================================================================
incremental: true

`sampleStream()` downloads streaming tweets of no particular topic.

```{r samplestream, eval=FALSE}
tweets_stream <- sampleStream("", timeout=10, oauth=twitCred)
```

`filterStream()` downloads streaming tweets based on search parameters.

```{r filterStream, eval=FALSE}
tweets_Trump  <- filterStream("", track="Trump", timeout=10, oauth=twitCred)
```

The first argument for both functions is the path to write tweets to. `""` writes
the tweets to the console. This permits assignment to an object.

streamR
====================================================================================
incremental: true

Once you've streamed data in you need to parse it from JSON format into a data frame 
for it to be particularly useful.

```{r parsing, eval=FALSE}
tweets_from_stream <- parseTweets(tweets_stream, verbose = FALSE)
as_tibble(tweets_from_stream) %>% arrange(followers_count) %>% select(name)
```

```{r real_parsing, eval=TRUE, echo=FALSE}
load("tweets_from_stream.RData")
as_tibble(tweets_from_stream) %>% arrange(followers_count) %>% select(name)
```


Using streamR
====================================================================================

Since it collects tweets in real time, the streaming API tends to be most useful when you leave it running *for a long time*.

You can use it to monitor events as they're happening, or to catch a "snapshot" in time.

I let it run for 1200 seconds to replicate an example by the package author, Pablo Barbera, 
in which tweets in the US are collected and then mapped in ggplot.


Using streamR
====================================================================================

```{r big_stream, eval=FALSE}
filterStream("tweetsUS.json", locations = c(-125, 25, -66, 50), timeout = 1200, 
             oauth = twitCred)
tweets.df <- parseTweets("tweetsUS.json", verbose = FALSE)
map.data <- map_data("state")
points <- data.frame(x=as.numeric(tweets.df$lon), y=as.numeric(tweets.df$lat))
points <- points[points$y > 25, ]
ggplot(map.data) + geom_map(aes(map_id=region), map=map.data, fill="white", 
                            color = "grey20", size = 0.25) + 
  expand_limits(x = map.data$long, y = map.data$lat) + 
  theme(axis.line = element_blank(), axis.text = element_blank(), 
        axis.ticks = element_blank(), axis.title = element_blank(), 
        panel.background = element_blank(), panel.border = element_blank(), 
        panel.grid.major = element_blank(), plot.background=element_blank(), 
        plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), "lines")) + 
  geom_point(data=points, aes(x=x, y=y), size=1, alpha=1/5, color="darkblue")
```


Using streamR
====================================================================================

```{r big_stream_actual, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, fig.width=11, fig.height=6, out.width="1100px", out.height="600px"}
load("twitter_map.Rdata")
twitter_map
```


Using Twitter Data
====================================================================================

Getting data from social media is easy with `SocialMediaLab` (it used to be hard!).

Typically we use these data for:

* Text mining (covered today)
* Network analysis (CS&SS567: Analysis of Social Network Data)


Example Network Graphs
====================================================================================

![https://github.com/vosonlab/SocialMediaLab](starwars_network.png)


Analyzing Twitter Text Data
====================================================================================

Large volumes of text data can be challenging to make sense of, so they are most useful when
approached using text analysis techniques.

But even they might not help with some kinds of text:

![](covfefe.PNG)



Text Mining with tm
====================================================================================
type: section


What is Text Mining?
====================================================================================

Text mining is a form of text analytics, that is a process for extracting information
from text intended to be read by humans.

Text mining can cover things like categorization of subjects, sentiment analysis, and
automated summarization.

Typically we want to take a very large amount of text---often from many different 
documents or sources---and produce useful numerical and textual summaries.

Text mining is very commonly done with content from social media data!


Text Mining Terminology
====================================================================================

* `tm`: R package for performing text mining
* **Term**: A word
* **Document**: A collection of *terms*
* **Corpus**: A collection of *documents* (plural: corpora)
* **Dictionary**: A set of relevant *terms*


My First Corpus
====================================================================================

We can make a basic corpus manually by creating a character vector, running `VectorSource()` on it to read it in, and then `VCorpus()` to corpus-ify:

```{r tm, warning=FALSE, message=FALSE}
library(tm)
UW_tweets <- c("Remembering and honoring those who made the ultimate sacrifice while serving our country. #MemorialDay2016",
               "VIDEO: This spring @UW students taught literacy arts to #Colville Reservation students. Check out book they made!",
               "Enjoy the long weekend, Huskies! And to those studying for finals: Good luck and hang in there!",
               ".@UWBuerk & @UWFosterSchoolâ€“hosted biz plan competition awards $85,000 to students for new ventures. http://ow.ly/3PtI300F87Y  #UWinnovates")
toy_corpus <- VCorpus(VectorSource(UW_tweets))
```


Accessing Corpus Entries
====================================================================================

A corpus is just a fancy list of documents, and you can access a document as a list entry:

```{r}
toy_corpus[[3]]
as.character(toy_corpus[[3]])
```


Text Files as Documents
====================================================================================

You will more likely be making corpora from sources like Twitter or reading in data from text files. 

We'll import a sample of emails from the [Enron corpus](http://bailando.sims.berkeley.edu/enron_email.html) assembled by UC Berkeley students. First, let's download a ZIP file with the text files and unzip it.

```{r eval=FALSE}
download.file("http://clanfear.github.io/CSSS508/Lectures/Week10/enron_sample_emails.zip", destfile = "enron_emails.zip", mode = "wb")
unzip("enron_emails.zip", exdir = "enron_emails")
```

```{r}
length(list.files("enron_emails/enron_sample_emails"))
```


Reading in Text Files
====================================================================================

Let's make a corpus where each document is an email in the Enron subsample:

```{r cache=TRUE}
enron_corpus <- VCorpus(DirSource(directory = "enron_emails/enron_sample_emails", mode = "text"))
as.character(enron_corpus[[3]])
```

Transformations (Maps)
====================================================================================
incremental: true

To prepare this text, we want to: 

1. Change text to lowercase
2. Remove: 
   * Stopwords (words that do not contain useful information)
   * Header terms
   * Punctuation
   * Numbers
   * Whitespace
3. "Stem" the words (strip prefixes and suffixes)

Then we can actually do analysis!


Transformations Syntax
====================================================================================

```{r cache=TRUE}
# install.packages("SnowballC") # may solve errors
enron_stripped <- enron_corpus %>%
    tm_map(content_transformer(str_to_lower)) %>%
    tm_map(removeWords, stopwords("english")) %>%
    tm_map(removeWords, 
        c("javamail.evans@thyme", "message-id", "date", "subject", 
         "mime-version", "content-type", "text/plain", "charset=us-ascii", 
         "content-transfer-encoding", "x-", "x-cc", "x-bcc", "x-folder", 
         "x-origin", "x-filename")) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(stripWhitespace) %>%
    tm_map(stemDocument)
```

Word Clouds
====================================================================================

```{r, fig.width = 11, fig.height = 4.4, dpi=300, out.width="1100px", out.height="440px"}
library(wordcloud); library(RColorBrewer)
wordcloud(enron_stripped, min.freq = 2, max.words = 80)
```


Filtering to Emails with "California"
====================================================================================

We'll write a function that takes the content of the documents and looks for any instance of `"california"`, then use it with `tm_filter()`:

```{r, cache=TRUE}
doc_word_search <- function(x, pattern) {
    any(str_detect(content(x), pattern = pattern))
}
cali_emails <- enron_stripped %>%
    tm_filter(doc_word_search, pattern = "california")
length(cali_emails)
```


Term-Document Matrices
====================================================================================

We can look for patterns across the documents by constructing a `TermDocumentMatrix()`:

```{r, cache=TRUE}
enron_tdm <- TermDocumentMatrix(enron_stripped)
str(enron_tdm)
```

What Does the Matrix Look Like?
====================================================================================

Too big to view at once, but we can look at snippets with `inspect()`:

```{r}
inspect(enron_tdm[1:5, 1:5])
```

Removing Sparse Words
====================================================================================

We could focus on words that appear in at least 40% of documents.

```{r}
enron_tdm_sparse <- removeSparseTerms(enron_tdm, 0.60)
inspect(enron_tdm_sparse)
```


Favorite Dictionary Words
====================================================================================

Or we can make make a term-document matrix focusing on words in a dictionary and look at just those columns:

```{r}
inspect(TermDocumentMatrix(enron_stripped, list(dictionary = c("california", "utah", "texas")))[, 1:5])
```


Most Frequent Words
====================================================================================

Which terms appear at least 200 times?

```{r}
findFreqTerms(enron_tdm, 200)
```


Word Associations
====================================================================================

Which words co-occur frequently with "california"?
```{r}
findAssocs(enron_tdm, "california", 0.90)
```


What Else Might You Do?
====================================================================================

* Use the `tidytext` package to work in a "tidy" way
* Make more visualizations of word frequencies or relationships in `ggplot2`
* Use hierarchical clustering to group together terms
* Fit topic models to find overarching topics
* Use `NLP` package to find bigrams (two-word phrases)
* Use `qdap` or `tidytext` packages to classify document sentiment (positive, negative) and look for relationships

Note: Before doing research with scraped data or social media data,
*make sure you are legally permitted to do so*. Some networks (e.g. *Nextdoor*), 
specifically ban collecting or analyzing data. Almost all websites will eventually
block you if you scrape *lots* of data.


Getting Serious with Text Mining
====================================================================================

I've introduced you to some basic text mining approaches.

If you want to *get serious* with text mining using modern techniques and "tidy" coding,
I suggest you look into the recently released [Text Mining with R](http://http://tidytextmining.com/),
written by the authors of the `tidytext` package.

Like *R for Data Science* and *Advanced R*, *Text Mining with R* is a free online textbook
(available in print also) that uses modern approaches to R coding to perform analyses
on real world data.


